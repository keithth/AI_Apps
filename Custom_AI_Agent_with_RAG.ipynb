{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keithth/AI_Apps/blob/main/Custom_AI_Agent_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Custom AI Agent with Haystack\n",
        "\n",
        "*Notebook by [Bilge Yucel](https://www.linkedin.com/in/bilge-yucel/) for Code & Deploy. Watch the recording [here](https://www.linkedin.com/events/code-deploy-buildyourfirstgenai7226658275792932864/comments/)*\n",
        "\n",
        "üìö Useful Resources\n",
        "* [üåê Website](https://haystack.deepset.ai/)\n",
        "* [üìò Documentation](https://docs.haystack.deepset.ai/docs)\n",
        "* [üßë‚Äçüè´ Tutorials](https://haystack.deepset.ai/tutorials)\n",
        "* [üßë‚Äçüç≥ Cookbooks](https://github.com/deepset-ai/haystack-cookbook)"
      ],
      "metadata": {
        "id": "oXOLE-gZ5Bz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Build an AI Agent with Haystack](#scrollTo=oXOLE-gZ5Bz9)\n",
        "\n",
        ">>[Tools ‚öíÔ∏è](#scrollTo=sj6RkPEd9Qyr)\n",
        "\n",
        ">>>[Indexing for RAG](#scrollTo=CLcmK85e96hL)\n",
        "\n",
        ">>[RAG](#scrollTo=mnOh6_0A-USp)\n",
        "\n",
        ">>[Web Search](#scrollTo=Jcql4NR8AveC)\n",
        "\n",
        ">>[Router](#scrollTo=nXpKuSz8B10i)\n",
        "\n",
        ">>[Agent ü§ñ](#scrollTo=YWP5qKNbExoj)\n",
        "\n",
        ">>[Live Demo](#scrollTo=C3Q5Ji1QRyeP)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "_KF11zZBVR9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key & logs"
      ],
      "metadata": {
        "id": "Gtv7PtzYtO7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve secrets directly from Colab's Secrets panel\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# os.environ[\"Unstructured_API_key\"] = userdata.get('Unstructured_API_key')\n",
        "os.environ[\"SERPERDEV_API_KEY\"] = userdata.get('SERPERDEV_API_KEY')\n",
        "# os.environ[\"HF_API_TOKEN\"] = userdata.get('HF_API_TOKEN')\n",
        "# os.environ[\"weather_api_key\"] = userdata.get('weather_api_key')\n",
        "\n",
        "print(\"All API keys have been loaded from secrets.\")\n"
      ],
      "metadata": {
        "id": "fXLUGX4C96Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "_nLpBnLGEt8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpO2BPEWm1ZI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -qU haystack-ai\n",
        "!pip install -qU trafilatura"
      ]
    },
    {
      "source": [
        "!pip show haystack-ai trafilatura | grep Version | cut -d: -f2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "E12oLXuML2_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import ChatMessage\n"
      ],
      "metadata": {
        "id": "dl1YDkyaXMO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "\n",
        "# Define the system message that includes the tool definitions\n",
        "system_message = ChatMessage.from_system(\n",
        "\"\"\"\n",
        "You are a virtual assistant, equipped with the following tools:\n",
        "\n",
        "\"tools\": [\n",
        "  {\n",
        "    \"name\": \"search_web\",\n",
        "    \"description\": \"Access to Google search, use this tool whenever information on recents events is needed\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"query\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"query to search in web\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"search_haystack\",\n",
        "    \"description\": \"Access to Haystack documentation, use this tool whenever information on building with LLMs, custom AI applications, Haystack, the open source LLM framework, is needed\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"query\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"query to search in the database\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "Select the most appropriate tool to resolve the user's query. Respond in JSON format, specifying the user request, modified query and the chosen tool for the response.\n",
        "If you can't match user query to an above listed tools, respond with `search_web`.\n",
        "\n",
        "Here are some examples:\n",
        "\n",
        "{\n",
        "  \"user_request\": \"Why did Elon Musk recently sue OpenAI?\",\n",
        "  \"tool_name\": \"search_web\",\n",
        "  \"query\": \"Why did Elon Musk recently sue OpenAI?\"\n",
        "}\n",
        "{\n",
        "  \"user_request\": \"What are the init parameters of HuggingFaceAPIGenerator component?\"\n",
        "  \"tool_name\": \"search_haystack\"\n",
        "  \"query\": \"What are the init parameters of HuggingFaceAPIGenerator component?\"\n",
        "}\n",
        "\n",
        "Choose the best tool (or none) for each user request and modify the query, considering the current context of the conversation specified above.\n",
        "\n",
        "Here's the user_request: {{query}}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Define the user message with the query\n",
        "user_message = ChatMessage.from_user(\" Where, in the Gulf of Mexico, is The Fuji system?\")\n",
        "\n",
        "# Initialize the OpenAI Chat Generator with your desired model\n",
        "chat_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "mxsMDC3gyzv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the system message content is a string\n",
        "# Using the from_system class method to create ChatMessage object\n",
        "system_msg = ChatMessage.from_system(system_message.text)\n",
        "\n",
        "response = chat_generator.run(messages=[\n",
        "    system_msg,\n",
        "    user_message\n",
        "])\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "PHQbhbnUTYlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(system_message.text))\n"
      ],
      "metadata": {
        "id": "THqLX5W7r58V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools ‚öíÔ∏è\n",
        "\n",
        "* RAG Pipeline (Indexing + Query)\n",
        "* Web Search\n",
        "* Weather API (Custom component)"
      ],
      "metadata": {
        "id": "sj6RkPEd9Qyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing for RAG"
      ],
      "metadata": {
        "id": "CLcmK85e96hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "# Indexing pipeline\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(instance=LinkContentFetcher(), name=\"fetcher\")\n",
        "indexing_pipeline.add_component(instance=HTMLToDocument(), name=\"converter\")\n",
        "indexing_pipeline.add_component(instance=DocumentCleaner(), name=\"cleaner\")\n",
        "indexing_pipeline.add_component(instance=DocumentSplitter(split_by=\"word\", split_length=512, split_overlap=50), name=\"splitter\")\n",
        "indexing_pipeline.add_component(instance=DocumentWriter(document_store = document_store), name=\"writer\")\n",
        "\n",
        "indexing_pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
        "indexing_pipeline.connect(\"converter.documents\", \"cleaner\")\n",
        "indexing_pipeline.connect(\"cleaner\", \"splitter\")\n",
        "indexing_pipeline.connect(\"splitter\", \"writer.documents\")\n",
        "\n",
        "# index some documentation pages to use for RAG\n",
        "indexing_pipeline.run({\n",
        "    \"fetcher\": {\n",
        "        \"urls\": [\"https://drive.google.com/file/d/1Fl-orR32Fy5ZTtNptPcKAo9chpw7NgoE/view?usp=drive_link\",\n",
        "            \"https://docs.haystack.deepset.ai/docs/intro\",\n",
        "            \"https://docs.haystack.deepset.ai/docs/huggingfacelocalgenerator\",\n",
        "            \"https://docs.haystack.deepset.ai/docs/huggingfacelocalchatgenerator\",\n",
        "            \"https://docs.haystack.deepset.ai/reference/generators-api\",\n",
        "            \"https://haystack.deepset.ai/overview/quick-start\",\n",
        "            \"https://haystack.deepset.ai/overview/intro\"\n",
        "            ]}})"
      ],
      "metadata": {
        "id": "2gl2W6wG6bDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "mnOh6_0A-USp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from pprint import pprint\n",
        "\n",
        "# RAG pipeline: initialize the retriever as before\n",
        "retriever = InMemoryBM25Retriever(document_store=document_store)\n",
        "\n",
        "# Define a prompt template suitable for OpenAI's Chat model.\n",
        "# Notice that we removed the <s>[INST] and [/INST] tokens.\n",
        "rag_prompt_template = \"\"\"You are a virtual assistant. Answer the following query based on the documents provided below.\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}\n",
        "\"\"\"\n",
        "\n",
        "# Create a prompt builder for the RAG task\n",
        "prompt_builder_for_rag = PromptBuilder(template=rag_prompt_template,\n",
        "                                       required_variables=[\"documents\", \"query\"]\n",
        "                                       )\n",
        "\n",
        "# Instead of using HuggingFaceAPIGenerator, initialize the OpenAIChatGenerator.\n",
        "llm_for_rag = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Example usage:\n",
        "# Suppose we build the prompt using the prompt builder:\n",
        "# changed from build_prompt to run and combined the arguments into a single dict\n",
        "prompt_text = prompt_builder_for_rag.run(\n",
        "    documents=[{\"content\": \"Example document content.\"}],\n",
        "    query=\"What is Natural Language Processing?\"\n",
        ")\n",
        "pprint(prompt_text)\n"
      ],
      "metadata": {
        "id": "WXd_eQRsLaJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate a response with OpenAI, you need to convert the prompt text into a chat message format.\n",
        "# Typically, you'll provide a system message with the prompt and a user message with the query.\n",
        "system_message = ChatMessage.from_system(prompt_text)\n",
        "user_message = ChatMessage.from_user(\"Please provide a concise answer.\")\n"
      ],
      "metadata": {
        "id": "ImaUVIKSvSjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(type(system_message.text))\n",
        "\n",
        "pprint(system_message.text.keys())\n",
        "\n",
        "plain_text = system_message.text.get(\"prompt\", \"\")\n",
        "print(plain_text)\n"
      ],
      "metadata": {
        "id": "b62DUfWUlx5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plain_text = system_message.text[\"content\"]\n"
      ],
      "metadata": {
        "id": "p3ZoPr-I8iBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_for_rag.run(messages=[\n",
        "    ChatMessage.from_system(system_message.text[\"prompt\"]),\n",
        "    user_message\n",
        "])\n",
        "pprint(response)"
      ],
      "metadata": {
        "id": "4ZkkBAjQ1KN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Search"
      ],
      "metadata": {
        "id": "Jcql4NR8AveC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.websearch.serper_dev import SerperDevWebSearch\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "\n",
        "# Web RAG pipeline using OpenAI\n",
        "\n",
        "# Define a prompt template without special tokens for OpenAI\n",
        "prompt_for_websearch = \"\"\"You are a virtual assistant that answers the following query based on documents retrieved from the web.\n",
        "Your answer should clearly indicate that it was generated from websearch.\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{ query }}\"\"\"\n",
        "\n",
        "pprint(prompt_for_websearch)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R3ZhdHrQDLkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the web search component (remains unchanged)\n",
        "websearch = SerperDevWebSearch()\n",
        "\n",
        "# Create the prompt builder for websearch\n",
        "prompt_builder_for_websearch = PromptBuilder(\n",
        "    template=prompt_for_websearch,\n",
        "    required_variables=[\"documents\", \"query\"]\n",
        ")\n",
        "\n",
        "# Initialize the OpenAI Chat Generator with your desired model (e.g., \"gpt-4\" or \"gpt-3.5-turbo\")\n",
        "llm_for_web = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Example usage:\n",
        "# Assume you have some documents retrieved by your websearch component and a query:\n",
        "documents = [{\"content\": \"Example web document content about recent AI research.\"}]\n",
        "query = \"What are the latest developments in AI research?\"\n",
        "\n",
        "# Build the prompt using the prompt builder\n",
        "# Build the prompt using the prompt builder\n",
        "built_prompt = prompt_builder_for_websearch.run(documents=documents, query=query)\n",
        "\n",
        "pprint(\"Built Prompt:\")\n",
        "pprint(built_prompt)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Iil_NXBA-9Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For OpenAIChatGenerator, convert the built prompt into a system message and add a user message\n",
        "from haystack.dataclasses import ChatMessage\n",
        "# Access the 'prompt' key from the dictionary\n",
        "system_message = ChatMessage.from_system(built_prompt[\"prompt\"])\n",
        "user_message = ChatMessage.from_user(\"Please provide a concise answer.\")"
      ],
      "metadata": {
        "id": "XjwCGnXYDqXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the generator with the chat messages\n",
        "response = llm_for_web.run(messages=[system_message, user_message])\n",
        "pprint(\"Response:\")\n",
        "pprint(response)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3I6V5pPAwCLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(system_message.text))\n",
        "\n",
        "# print(system_message.text.keys())\n",
        "\n",
        "# plain_text = system_message.text.get(\"prompt\", \"\")\n",
        "# print(plain_text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s3xqBHJ6Pda-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Router"
      ],
      "metadata": {
        "id": "nXpKuSz8B10i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.routers import ConditionalRouter\n",
        "\n",
        "# Router\n",
        "main_routes = [\n",
        "       {\n",
        "        \"condition\": \"{{'search_haystack' in tool_name}}\",\n",
        "        \"output\": \"{{query}}\",\n",
        "        \"output_name\": \"search_haystack\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    {\n",
        "        \"condition\": \"{{'search_web' in tool_name}}\",\n",
        "        \"output\": \"{{query}}\",\n",
        "        \"output_name\": \"search_web\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "]\n",
        "\n",
        "tool_router = ConditionalRouter(main_routes)"
      ],
      "metadata": {
        "id": "N_dsH0i9BBG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connection Components"
      ],
      "metadata": {
        "id": "3DN63mZsWWWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import component\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "## AnswerParser\n",
        "@component\n",
        "class AnswerParser:\n",
        "\n",
        "  @component.output_types(tool_name=str, query=str)\n",
        "  def run(self, replies:List[str]):\n",
        "    reply = json.loads(replies[0])\n",
        "\n",
        "    tool_name = reply[\"tool_name\"]\n",
        "    query = reply[\"query\"]\n",
        "\n",
        "    return {\"tool_name\": tool_name, \"query\":query}"
      ],
      "metadata": {
        "id": "JFX9m39JD1fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent ü§ñ\n"
      ],
      "metadata": {
        "id": "YWP5qKNbExoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "# Changed import statement to reflect new location\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "# from haystack.components.messages import ChatMessage\n",
        "from haystack.dataclasses import ChatMessage\n",
        "\n",
        "\n",
        "\n",
        "# Define prompt_builder_for_agent\n",
        "prompt_builder_for_agent = PromptBuilder(\n",
        "    template=\"\"\"\n",
        "    Given the following user query, determine which tool to use.\n",
        "    If the tool is for searching the internet, the tool_name should be \"search_web\".\n",
        "    If the tool is for searching the document store, the tool_name should be \"search_haystack\".\n",
        "    The response should be a JSON object with \"tool_name\" and \"query\" as keys.\n",
        "\n",
        "    User query: {{query}}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Define llm_for_agent\n",
        "llm_for_agent = OpenAIChatGenerator(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define prompt_builder_for_websearch\n",
        "prompt_builder_for_websearch = PromptBuilder(\n",
        "    template=\"\"\"\n",
        "    Given the information below:\n",
        "    {% for document in documents %}\n",
        "    {{ document.content }}\n",
        "    {% endfor %}\n",
        "    Answer question: {{ query }}.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Define llm_for_web\n",
        "llm_for_web = OpenAIChatGenerator(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "agent = Pipeline()\n",
        "agent.add_component(\"prompt_builder_for_agent\", prompt_builder_for_agent)\n",
        "agent.add_component(\"llm_for_agent\", llm_for_agent)\n",
        "agent.add_component(\"answer_parser\", AnswerParser())\n",
        "agent.add_component(\"tool_router\", tool_router)\n",
        "\n",
        "# web\n",
        "agent.add_component(\"websearch\", websearch)\n",
        "agent.add_component(\"prompt_builder_for_websearch\", prompt_builder_for_websearch)\n",
        "agent.add_component(\"llm_for_web\", llm_for_web)\n",
        "\n",
        "# RAG\n",
        "agent.add_component(\"retriever\", retriever)\n",
        "agent.add_component(\"prompt_builder_for_rag\", prompt_builder_for_rag)\n",
        "agent.add_component(\"llm_for_rag\", llm_for_rag)\n",
        "\n",
        "# Web Search\n",
        "agent.connect(\"tool_router.search_web\", \"websearch.query\")\n",
        "agent.connect(\"tool_router.search_web\", \"prompt_builder_for_websearch.query\")\n",
        "agent.connect(\"websearch.documents\", \"prompt_builder_for_websearch.documents\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "evwocwMqxjoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The following connect function is corrected: the previous was using prompt_builder_for_websearch as sender, which is not correct\n",
        "# it needs to use the output of prompt_builder_for_websearch, called prompt and transform it to ChatMessage so it can be passed to llm_for_web\n",
        "agent.connect(\"prompt_builder_for_websearch.prompt\", \"llm_for_web.messages\",  transform=lambda x: [ChatMessage(role=\"user\", content=x)])\n"
      ],
      "metadata": {
        "id": "Fw8Oymu0Z0yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Incorrect usage (pre-2.10 style):\n",
        "# agent.connect(\"prompt_builder_for_websearch.prompt\", \"llm_for_web.messages\", transform=lambda x: [ChatMessage(role=\"user\", content=x)])\n",
        "\n",
        "# Correct usage in Haystack 2.10:\n",
        "agent.connect(\"prompt_builder_for_websearch.prompt\", \"llm_for_web.messages\")\n"
      ],
      "metadata": {
        "id": "vpmKSuY6Z_nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# RAG\n",
        "agent.connect(\"tool_router.search_haystack\", \"retriever.query\")\n",
        "agent.connect(\"tool_router.search_haystack\", \"prompt_builder_for_rag.query\")\n",
        "agent.connect(\"retriever.documents\", \"prompt_builder_for_rag.documents\")\n",
        "agent.connect(\"prompt_builder_for_rag\", \"llm_for_rag\")"
      ],
      "metadata": {
        "id": "0izh53B5Z5Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.show()"
      ],
      "metadata": {
        "id": "CLgjcScfwSSS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you tell me why cats purr?\"\n",
        "agent.run({\"prompt_builder_for_agent\": {\"query\": query}}, include_outputs_from={\"llm_for_agent\"})"
      ],
      "metadata": {
        "id": "ukDRCPBSwb5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Live Demo"
      ],
      "metadata": {
        "id": "C3Q5Ji1QRyeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade gradio"
      ],
      "metadata": {
        "id": "6wWyy8mdMfXG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatbot(message, history):\n",
        "    response = agent.run({\"prompt_builder_for_agent\": {\"query\": message}}, include_outputs_from={\"tool_router\", \"answer_parser\"})\n",
        "    answer = \"\"\n",
        "\n",
        "    if \"llm_for_web\" in response.keys():\n",
        "      answer = response[\"llm_for_web\"][\"replies\"][0]\n",
        "    elif \"llm_for_rag\" in response.keys():\n",
        "      answer = response[\"llm_for_rag\"][\"replies\"][0]\n",
        "    else:\n",
        "      answer = response[\"get_weather\"][\"text\"]\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chatbot,\n",
        "    examples=[\n",
        "        \"What's Einstein's first name?\",\n",
        "        \"How is the weather in Toronto?\",\n",
        "        \"Why do cats purr?\",\n",
        "        \"What is Haystack?\"\n",
        "    ],\n",
        "    title=\"Ask me about anything!\",\n",
        ")"
      ],
      "metadata": {
        "id": "k5tIJ9t3Sk_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "G3Cy-mL5xqSK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
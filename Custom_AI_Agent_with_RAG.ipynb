{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keithth/AI_Apps/blob/main/Custom_AI_Agent_with_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Custom AI Agent with Haystack\n",
        "\n",
        "*Notebook by [Bilge Yucel](https://www.linkedin.com/in/bilge-yucel/) for Code & Deploy. Watch the recording [here](https://www.linkedin.com/events/code-deploy-buildyourfirstgenai7226658275792932864/comments/)*\n",
        "\n",
        "📚 Useful Resources\n",
        "* [🌐 Website](https://haystack.deepset.ai/)\n",
        "* [📘 Documentation](https://docs.haystack.deepset.ai/docs)\n",
        "* [🧑‍🏫 Tutorials](https://haystack.deepset.ai/tutorials)\n",
        "* [🧑‍🍳 Cookbooks](https://github.com/deepset-ai/haystack-cookbook)"
      ],
      "metadata": {
        "id": "oXOLE-gZ5Bz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Build an AI Agent with Haystack](#scrollTo=oXOLE-gZ5Bz9)\n",
        "\n",
        ">>[Tools ⚒️](#scrollTo=sj6RkPEd9Qyr)\n",
        "\n",
        ">>>[Indexing for RAG](#scrollTo=CLcmK85e96hL)\n",
        "\n",
        ">>[RAG](#scrollTo=mnOh6_0A-USp)\n",
        "\n",
        ">>[Web Search](#scrollTo=Jcql4NR8AveC)\n",
        "\n",
        ">>[Router](#scrollTo=nXpKuSz8B10i)\n",
        "\n",
        ">>[Agent 🤖](#scrollTo=YWP5qKNbExoj)\n",
        "\n",
        ">>[Live Demo](#scrollTo=C3Q5Ji1QRyeP)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "_KF11zZBVR9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key & logs"
      ],
      "metadata": {
        "id": "Gtv7PtzYtO7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve secrets directly from Colab's Secrets panel\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "# os.environ[\"Unstructured_API_key\"] = userdata.get('Unstructured_API_key')\n",
        "os.environ[\"SERPERDEV_API_KEY\"] = userdata.get('SERPERDEV_API_KEY')\n",
        "# os.environ[\"HF_API_TOKEN\"] = userdata.get('HF_API_TOKEN')\n",
        "# os.environ[\"weather_api_key\"] = userdata.get('weather_api_key')\n",
        "\n",
        "print(\"All API keys have been loaded from secrets.\")\n"
      ],
      "metadata": {
        "id": "fXLUGX4C96Sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfa4e88-6a4f-4d7e-9e8c-fb21c4dc2620"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All API keys have been loaded from secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "_nLpBnLGEt8S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FpO2BPEWm1ZI",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -qU haystack-ai\n",
        "!pip install -qU trafilatura"
      ]
    },
    {
      "source": [
        "!pip show haystack-ai trafilatura | grep Version | cut -d: -f2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "E12oLXuML2_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c17c25-32ff-41fc-fc91-bd4f819aa48f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2.10.3\n",
            " 2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import ChatMessage\n"
      ],
      "metadata": {
        "id": "dl1YDkyaXMO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "\n",
        "# Define the system message that includes the tool definitions\n",
        "system_message = ChatMessage.from_system(\n",
        "\"\"\"\n",
        "You are a virtual assistant, equipped with the following tools:\n",
        "\n",
        "\"tools\": [\n",
        "  {\n",
        "    \"name\": \"search_web\",\n",
        "    \"description\": \"Access to Google search, use this tool whenever information on recents events is needed\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"query\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"query to search in web\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    \"name\": \"search_haystack\",\n",
        "    \"description\": \"Access to Haystack documentation, use this tool whenever information on building with LLMs, custom AI applications, Haystack, the open source LLM framework, is needed\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"query\": {\n",
        "          \"type\": \"string\",\n",
        "          \"description\": \"query to search in the database\"\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "Select the most appropriate tool to resolve the user's query. Respond in JSON format, specifying the user request, modified query and the chosen tool for the response.\n",
        "If you can't match user query to an above listed tools, respond with `search_web`.\n",
        "\n",
        "Here are some examples:\n",
        "\n",
        "{\n",
        "  \"user_request\": \"Why did Elon Musk recently sue OpenAI?\",\n",
        "  \"tool_name\": \"search_web\",\n",
        "  \"query\": \"Why did Elon Musk recently sue OpenAI?\"\n",
        "}\n",
        "{\n",
        "  \"user_request\": \"What are the init parameters of HuggingFaceAPIGenerator component?\"\n",
        "  \"tool_name\": \"search_haystack\"\n",
        "  \"query\": \"What are the init parameters of HuggingFaceAPIGenerator component?\"\n",
        "}\n",
        "\n",
        "Choose the best tool (or none) for each user request and modify the query, considering the current context of the conversation specified above.\n",
        "\n",
        "Here's the user_request: {{query}}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Define the user message with the query\n",
        "user_message = ChatMessage.from_user(\" Where, in the Gulf of Mexico, is The Fuji system?\")\n",
        "\n",
        "# Initialize the OpenAI Chat Generator with your desired model\n",
        "chat_generator = OpenAIChatGenerator(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "mxsMDC3gyzv-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure that the system message content is a string\n",
        "# Using the from_system class method to create ChatMessage object\n",
        "system_msg = ChatMessage.from_system(system_message.text)\n",
        "\n",
        "response = chat_generator.run(messages=[\n",
        "    system_msg,\n",
        "    user_message\n",
        "])\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "PHQbhbnUTYlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f358842-a10e-44bd-8878-1b02ad01130c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'replies': [ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>, _content=[TextContent(text='{\\n  \"user_request\": \"Where, in the Gulf of Mexico, is The Fuji system?\",\\n  \"tool_name\": \"search_web\",\\n  \"query\": \"Where in the Gulf of Mexico is The Fuji system?\"\\n}')], _name=None, _meta={'model': 'gpt-4o-mini-2024-07-18', 'index': 0, 'finish_reason': 'stop', 'usage': {'completion_tokens': 47, 'prompt_tokens': 404, 'total_tokens': 451, 'completion_tokens_details': CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), 'prompt_tokens_details': PromptTokensDetails(audio_tokens=0, cached_tokens=0)}})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(system_message.text))\n"
      ],
      "metadata": {
        "id": "THqLX5W7r58V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae75f9b-1555-4b7f-9371-3f9145dd3078"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools ⚒️\n",
        "\n",
        "* RAG Pipeline (Indexing + Query)\n",
        "* Web Search\n",
        "* Weather API (Custom component)"
      ],
      "metadata": {
        "id": "sj6RkPEd9Qyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing for RAG"
      ],
      "metadata": {
        "id": "CLcmK85e96hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "# Indexing pipeline\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(instance=LinkContentFetcher(), name=\"fetcher\")\n",
        "indexing_pipeline.add_component(instance=HTMLToDocument(), name=\"converter\")\n",
        "indexing_pipeline.add_component(instance=DocumentCleaner(), name=\"cleaner\")\n",
        "indexing_pipeline.add_component(instance=DocumentSplitter(split_by=\"word\", split_length=512, split_overlap=50), name=\"splitter\")\n",
        "indexing_pipeline.add_component(instance=DocumentWriter(document_store = document_store), name=\"writer\")\n",
        "\n",
        "indexing_pipeline.connect(\"fetcher.streams\", \"converter.sources\")\n",
        "indexing_pipeline.connect(\"converter.documents\", \"cleaner\")\n",
        "indexing_pipeline.connect(\"cleaner\", \"splitter\")\n",
        "indexing_pipeline.connect(\"splitter\", \"writer.documents\")\n",
        "\n",
        "# index some documentation pages to use for RAG\n",
        "indexing_pipeline.run({\n",
        "    \"fetcher\": {\n",
        "        \"urls\": [\"https://drive.google.com/file/d/1Fl-orR32Fy5ZTtNptPcKAo9chpw7NgoE/view?usp=drive_link\",\n",
        "            \"https://docs.haystack.deepset.ai/docs/intro\",\n",
        "            \"https://docs.haystack.deepset.ai/docs/huggingfacelocalgenerator\",\n",
        "            \"https://docs.haystack.deepset.ai/docs/huggingfacelocalchatgenerator\",\n",
        "            \"https://docs.haystack.deepset.ai/reference/generators-api\",\n",
        "            \"https://haystack.deepset.ai/overview/quick-start\",\n",
        "            \"https://haystack.deepset.ai/overview/intro\"\n",
        "            ]}})"
      ],
      "metadata": {
        "id": "2gl2W6wG6bDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff0af45-6ce7-40a3-e073-804788eb7a3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.core.pipeline.base:Warming up component splitter...\n",
            "INFO:haystack.core.pipeline.pipeline:Running component fetcher\n",
            "INFO:haystack.core.pipeline.pipeline:Running component converter\n",
            "INFO:haystack.core.pipeline.pipeline:Running component cleaner\n",
            "INFO:haystack.core.pipeline.pipeline:Running component splitter\n",
            "INFO:haystack.core.pipeline.pipeline:Running component writer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'writer': {'documents_written': 18}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "mnOh6_0A-USp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from pprint import pprint\n",
        "\n",
        "# RAG pipeline: initialize the retriever as before\n",
        "retriever = InMemoryBM25Retriever(document_store=document_store)\n",
        "\n",
        "# Define a prompt template suitable for OpenAI's Chat model.\n",
        "# Notice that we removed the <s>[INST] and [/INST] tokens.\n",
        "rag_prompt_template = \"\"\"You are a virtual assistant. Answer the following query based on the documents provided below.\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{document.content}}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{query}}\n",
        "\"\"\"\n",
        "\n",
        "# Create a prompt builder for the RAG task\n",
        "prompt_builder_for_rag = PromptBuilder(template=rag_prompt_template,\n",
        "                                       required_variables=[\"documents\", \"query\"]\n",
        "                                       )\n",
        "\n",
        "# Instead of using HuggingFaceAPIGenerator, initialize the OpenAIChatGenerator.\n",
        "llm_for_rag = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Example usage:\n",
        "# Suppose we build the prompt using the prompt builder:\n",
        "# changed from build_prompt to run and combined the arguments into a single dict\n",
        "prompt_text = prompt_builder_for_rag.run(\n",
        "    documents=[{\"content\": \"Example document content.\"}],\n",
        "    query=\"What is Natural Language Processing?\"\n",
        ")\n",
        "pprint(prompt_text)\n"
      ],
      "metadata": {
        "id": "WXd_eQRsLaJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a294de23-659b-4368-fa68-ac3a95c2d962"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': 'You are a virtual assistant. Answer the following query based on '\n",
            "           'the documents provided below.\\n'\n",
            "           '\\n'\n",
            "           'Documents:\\n'\n",
            "           '\\n'\n",
            "           '  Example document content.\\n'\n",
            "           '\\n'\n",
            "           '\\n'\n",
            "           'Query: What is Natural Language Processing?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate a response with OpenAI, you need to convert the prompt text into a chat message format.\n",
        "# Typically, you'll provide a system message with the prompt and a user message with the query.\n",
        "system_message = ChatMessage.from_system(prompt_text)\n",
        "user_message = ChatMessage.from_user(\"Please provide a concise answer.\")\n"
      ],
      "metadata": {
        "id": "ImaUVIKSvSjS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(type(system_message.text))\n",
        "\n",
        "pprint(system_message.text.keys())\n",
        "\n",
        "plain_text = system_message.text.get(\"prompt\", \"\")\n",
        "print(plain_text)\n"
      ],
      "metadata": {
        "id": "b62DUfWUlx5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2713627d-c5f7-4a04-97eb-41adf7e12281"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys(['prompt'])\n",
            "You are a virtual assistant. Answer the following query based on the documents provided below.\n",
            "\n",
            "Documents:\n",
            "\n",
            "  Example document content.\n",
            "\n",
            "\n",
            "Query: What is Natural Language Processing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plain_text = system_message.text[\"content\"]\n"
      ],
      "metadata": {
        "id": "p3ZoPr-I8iBI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm_for_rag.run(messages=[\n",
        "    ChatMessage.from_system(system_message.text[\"prompt\"]),\n",
        "    user_message\n",
        "])\n",
        "pprint(response)"
      ],
      "metadata": {
        "id": "4ZkkBAjQ1KN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c127a8-1c05-470d-f29d-54dc7d293f4e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'replies': [ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>,\n",
            "                         _content=[TextContent(text='Natural Language '\n",
            "                                                    'Processing (NLP) is a '\n",
            "                                                    'field of artificial '\n",
            "                                                    'intelligence that focuses '\n",
            "                                                    'on the interaction '\n",
            "                                                    'between computers and '\n",
            "                                                    'humans through natural '\n",
            "                                                    'language. It involves the '\n",
            "                                                    'ability of machines to '\n",
            "                                                    'understand, interpret, '\n",
            "                                                    'and respond to human '\n",
            "                                                    'language in a meaningful '\n",
            "                                                    'way, enabling '\n",
            "                                                    'applications such as '\n",
            "                                                    'speech recognition, '\n",
            "                                                    'sentiment analysis, and '\n",
            "                                                    'language translation.')],\n",
            "                         _name=None,\n",
            "                         _meta={'finish_reason': 'stop',\n",
            "                                'index': 0,\n",
            "                                'model': 'gpt-4o-mini-2024-07-18',\n",
            "                                'usage': {'completion_tokens': 62,\n",
            "                                          'completion_tokens_details': CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0),\n",
            "                                          'prompt_tokens': 49,\n",
            "                                          'prompt_tokens_details': PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n",
            "                                          'total_tokens': 111}})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Search"
      ],
      "metadata": {
        "id": "Jcql4NR8AveC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.websearch.serper_dev import SerperDevWebSearch\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "\n",
        "# Web RAG pipeline using OpenAI\n",
        "\n",
        "# Define a prompt template without special tokens for OpenAI\n",
        "prompt_for_websearch = \"\"\"You are a virtual assistant that answers the following query based on documents retrieved from the web.\n",
        "Your answer should clearly indicate that it was generated from websearch.\n",
        "\n",
        "Documents:\n",
        "{% for document in documents %}\n",
        "  {{ document.content }}\n",
        "{% endfor %}\n",
        "\n",
        "Query: {{ query }}\"\"\"\n",
        "\n",
        "pprint(prompt_for_websearch)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R3ZhdHrQDLkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34eacd71-dd14-47fb-9c65-f71daee88f8e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('You are a virtual assistant that answers the following query based on '\n",
            " 'documents retrieved from the web.\\n'\n",
            " 'Your answer should clearly indicate that it was generated from websearch.\\n'\n",
            " '\\n'\n",
            " 'Documents:\\n'\n",
            " '{% for document in documents %}\\n'\n",
            " '  {{ document.content }}\\n'\n",
            " '{% endfor %}\\n'\n",
            " '\\n'\n",
            " 'Query: {{ query }}')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the web search component (remains unchanged)\n",
        "websearch = SerperDevWebSearch()\n",
        "\n",
        "# Create the prompt builder for websearch\n",
        "prompt_builder_for_websearch = PromptBuilder(\n",
        "    template=prompt_for_websearch,\n",
        "    required_variables=[\"documents\", \"query\"]\n",
        ")\n",
        "\n",
        "# Initialize the OpenAI Chat Generator with your desired model (e.g., \"gpt-4\" or \"gpt-3.5-turbo\")\n",
        "llm_for_web = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Example usage:\n",
        "# Assume you have some documents retrieved by your websearch component and a query:\n",
        "documents = [{\"content\": \"Example web document content about recent AI research.\"}]\n",
        "query = \"What are the latest developments in AI research?\"\n",
        "\n",
        "# Build the prompt using the prompt builder\n",
        "# Build the prompt using the prompt builder\n",
        "built_prompt = prompt_builder_for_websearch.run(documents=documents, query=query)\n",
        "\n",
        "pprint(\"Built Prompt:\")\n",
        "pprint(built_prompt)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Iil_NXBA-9Xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9c0b96-29e4-49ea-e82f-0589dbfcdda6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Built Prompt:'\n",
            "{'prompt': 'You are a virtual assistant that answers the following query based '\n",
            "           'on documents retrieved from the web.\\n'\n",
            "           'Your answer should clearly indicate that it was generated from '\n",
            "           'websearch.\\n'\n",
            "           '\\n'\n",
            "           'Documents:\\n'\n",
            "           '\\n'\n",
            "           '  Example web document content about recent AI research.\\n'\n",
            "           '\\n'\n",
            "           '\\n'\n",
            "           'Query: What are the latest developments in AI research?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For OpenAIChatGenerator, convert the built prompt into a system message and add a user message\n",
        "from haystack.dataclasses import ChatMessage\n",
        "# Access the 'prompt' key from the dictionary\n",
        "system_message = ChatMessage.from_system(built_prompt[\"prompt\"])\n",
        "user_message = ChatMessage.from_user(\"Please provide a concise answer.\")"
      ],
      "metadata": {
        "id": "XjwCGnXYDqXW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run the generator with the chat messages\n",
        "response = llm_for_web.run(messages=[system_message, user_message])\n",
        "pprint(\"Response:\")\n",
        "pprint(response)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3I6V5pPAwCLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc009cf-b956-49e0-c8c6-76b3f6090332"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Response:'\n",
            "{'replies': [ChatMessage(_role=<ChatRole.ASSISTANT: 'assistant'>,\n",
            "                         _content=[TextContent(text='Recent developments in AI '\n",
            "                                                    'research include '\n",
            "                                                    'advancements in natural '\n",
            "                                                    'language processing, '\n",
            "                                                    'particularly with models '\n",
            "                                                    'achieving more human-like '\n",
            "                                                    'understanding and '\n",
            "                                                    'generation of text. '\n",
            "                                                    'Researchers are also '\n",
            "                                                    \"exploring AI's \"\n",
            "                                                    'applications in '\n",
            "                                                    'healthcare, such as '\n",
            "                                                    'disease diagnosis and '\n",
            "                                                    'personalized medicine. '\n",
            "                                                    'Additionally, there are '\n",
            "                                                    'ongoing efforts to '\n",
            "                                                    \"improve AI's ethical \"\n",
            "                                                    'frameworks and reduce '\n",
            "                                                    'biases in algorithms. '\n",
            "                                                    'Collaborative AI models '\n",
            "                                                    'that enhance multi-agent '\n",
            "                                                    'learning are also gaining '\n",
            "                                                    'traction, indicating a '\n",
            "                                                    'shift towards more '\n",
            "                                                    'interactive AI systems. '\n",
            "                                                    'These insights were '\n",
            "                                                    'gathered from recent web '\n",
            "                                                    'research.')],\n",
            "                         _name=None,\n",
            "                         _meta={'finish_reason': 'stop',\n",
            "                                'index': 0,\n",
            "                                'model': 'gpt-4o-mini-2024-07-18',\n",
            "                                'usage': {'completion_tokens': 93,\n",
            "                                          'completion_tokens_details': CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0),\n",
            "                                          'prompt_tokens': 71,\n",
            "                                          'prompt_tokens_details': PromptTokensDetails(audio_tokens=0, cached_tokens=0),\n",
            "                                          'total_tokens': 164}})]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(system_message.text))\n",
        "\n",
        "# print(system_message.text.keys())\n",
        "\n",
        "# plain_text = system_message.text.get(\"prompt\", \"\")\n",
        "# print(plain_text)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "s3xqBHJ6Pda-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd94697-7c37-49f0-80db-08de8ab9daa8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Router"
      ],
      "metadata": {
        "id": "nXpKuSz8B10i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.routers import ConditionalRouter\n",
        "\n",
        "# Router\n",
        "main_routes = [\n",
        "       {\n",
        "        \"condition\": \"{{'search_haystack' in tool_name}}\",\n",
        "        \"output\": \"{{query}}\",\n",
        "        \"output_name\": \"search_haystack\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "    {\n",
        "        \"condition\": \"{{'search_web' in tool_name}}\",\n",
        "        \"output\": \"{{query}}\",\n",
        "        \"output_name\": \"search_web\",\n",
        "        \"output_type\": str,\n",
        "    },\n",
        "]\n",
        "\n",
        "tool_router = ConditionalRouter(main_routes)"
      ],
      "metadata": {
        "id": "N_dsH0i9BBG6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connection Components"
      ],
      "metadata": {
        "id": "3DN63mZsWWWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent 🤖\n"
      ],
      "metadata": {
        "id": "YWP5qKNbExoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "from haystack.components.builders import PromptBuilder\n",
        "# Changed import statement to reflect new location\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "# from haystack.components.messages import ChatMessage\n",
        "from haystack.dataclasses import ChatMessage\n",
        "\n",
        "\n",
        "\n",
        "# Define prompt_builder_for_agent\n",
        "prompt_builder_for_agent = PromptBuilder(\n",
        "    template=\"\"\"\n",
        "    Given the following user query, determine which tool to use.\n",
        "    If the tool is for searching the internet, the tool_name should be \"search_web\".\n",
        "    If the tool is for searching the document store, the tool_name should be \"search_haystack\".\n",
        "    The response should be a JSON object with \"tool_name\" and \"query\" as keys.\n",
        "\n",
        "    User query: {{query}}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Define llm_for_agent\n",
        "llm_for_agent = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n",
        "\n",
        "# Define prompt_builder_for_websearch\n",
        "prompt_builder_for_websearch = PromptBuilder(\n",
        "    template=\"\"\"\n",
        "    Given the information below:\n",
        "    {% for document in documents %}\n",
        "    {{ document.content }}\n",
        "    {% endfor %}\n",
        "    Answer question: {{ query }}.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Define llm_for_web\n",
        "llm_for_web = OpenAIChatGenerator(model=\"gpt-4o-mini\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "evwocwMqxjoB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import component\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "## AnswerParser\n",
        "@component\n",
        "class AnswerParser:\n",
        "\n",
        "  @component.output_types(tool_name=str, query=str)\n",
        "  def run(self, replies:List[str]):\n",
        "    reply = json.loads(replies[0])\n",
        "\n",
        "    tool_name = reply[\"tool_name\"]\n",
        "    query = reply[\"query\"]\n",
        "\n",
        "    return {\"tool_name\": tool_name, \"query\":query}"
      ],
      "metadata": {
        "id": "JFX9m39JD1fH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Tuple\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack import component\n",
        "\n",
        "@component\n",
        "class PromptToChatMessageConverter:\n",
        "    \"\"\"\n",
        "    A custom component that converts a prompt string into a list of ChatMessage objects.\n",
        "    \"\"\"\n",
        "    @component.output_types(messages=List[ChatMessage])\n",
        "    def run(self, prompt: str) -> Tuple[Dict[str, List[ChatMessage]], str]:\n",
        "        # Convert the input prompt string into a list containing one system ChatMessage.\n",
        "        messages = [ChatMessage.from_system(prompt)]\n",
        "        return {\"messages\": messages}, \"output_1\"\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "IODlvNzFxPSp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack import Pipeline\n",
        "\n",
        "# Create the pipeline object.\n",
        "agent = Pipeline()\n",
        "\n",
        "# --- Add Agent Components ---\n",
        "agent.add_component(\"prompt_builder_for_agent\", prompt_builder_for_agent)\n",
        "agent.add_component(\"prompt_to_chat\", PromptToChatMessageConverter())\n",
        "agent.add_component(\"llm_for_agent\", llm_for_agent)\n",
        "agent.add_component(\"answer_parser\", AnswerParser())\n",
        "agent.add_component(\"tool_router\", tool_router)\n",
        "\n",
        "# --- Add Web Search Components ---\n",
        "agent.add_component(\"websearch\", websearch)\n",
        "agent.add_component(\"prompt_builder_for_websearch\", prompt_builder_for_websearch)\n",
        "agent.add_component(\"llm_for_web\", llm_for_web)\n",
        "\n",
        "# --- Add RAG Components ---\n",
        "agent.add_component(\"retriever\", retriever)\n",
        "agent.add_component(\"prompt_builder_for_rag\", prompt_builder_for_rag)\n",
        "agent.add_component(\"llm_for_rag\", llm_for_rag)\n",
        "\n",
        "# --- Connect Components ---\n",
        "# Web Search connections:\n",
        "agent.connect(\"tool_router.search_web\", \"websearch.query\")\n",
        "agent.connect(\"tool_router.search_web\", \"prompt_builder_for_websearch.query\")\n",
        "agent.connect(\"websearch.documents\", \"prompt_builder_for_websearch.documents\")\n",
        "# Connect the output from prompt_builder_for_websearch (a string) to our custom converter.\n",
        "agent.connect(\"prompt_builder_for_websearch.prompt\", \"prompt_to_chat.prompt\")\n",
        "# Connect the converted messages (a list of ChatMessage objects) to the LLM for web search.\n",
        "agent.connect(\"prompt_to_chat.messages\", \"llm_for_web.messages\")\n",
        "\n",
        "print(\"Pipeline setup complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y8t6V2Kxfmo",
        "outputId": "93c58115-3370-4fef-c388-ef78c986c499"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b1"
      ],
      "metadata": {
        "id": "EIxXbbFsADCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Live Demo"
      ],
      "metadata": {
        "id": "C3Q5Ji1QRyeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade gradio"
      ],
      "metadata": {
        "id": "6wWyy8mdMfXG",
        "collapsed": true
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatbot(message, history):\n",
        "    response = agent.run({\"prompt_builder_for_agent\": {\"query\": message}}, include_outputs_from={\"tool_router\", \"answer_parser\"})\n",
        "    answer = \"\"\n",
        "\n",
        "    if \"llm_for_web\" in response.keys():\n",
        "      answer = response[\"llm_for_web\"][\"replies\"][0]\n",
        "    elif \"llm_for_rag\" in response.keys():\n",
        "      answer = response[\"llm_for_rag\"][\"replies\"][0]\n",
        "    else:\n",
        "      answer = response[\"get_weather\"][\"text\"]\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chatbot,\n",
        "    examples=[\n",
        "        \"What's Einstein's first name?\",\n",
        "        \"How is the weather in Toronto?\",\n",
        "        \"Why do cats purr?\",\n",
        "        \"What is Haystack?\"\n",
        "    ],\n",
        "    title=\"Ask me about anything!\",\n",
        ")"
      ],
      "metadata": {
        "id": "k5tIJ9t3Sk_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee0daac-0f3f-4e8c-849a-d0086fd20694"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "id": "G3Cy-mL5xqSK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "d118159c-c914-4d74-a764-f8bfc49322ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2e090248b5626117d1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2e090248b5626117d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ]
}